{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EdgeConnect_Attention(downsampling)_BMVC2018.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "78bf9d21e056484d8c45beedd50a6862": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_98d9db8826d24817a6603130f699aaed",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_448d38e984794a698bc9ecc7236533ed",
              "IPY_MODEL_751e34b8783e4b5e9626ed5c042b6f73"
            ]
          }
        },
        "98d9db8826d24817a6603130f699aaed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "448d38e984794a698bc9ecc7236533ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4336e65e1db948dfb8d9b04521ab08af",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cfdbc7486f8340e0aa66e4670c4af66d"
          }
        },
        "751e34b8783e4b5e9626ed5c042b6f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ba57aec5e6d24d718707555f2666e723",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1999642624/? [06:50&lt;00:00, 4539307.28it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cf9cabb7482640e7bf1bb4140c48ffc2"
          }
        },
        "4336e65e1db948dfb8d9b04521ab08af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cfdbc7486f8340e0aa66e4670c4af66d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba57aec5e6d24d718707555f2666e723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cf9cabb7482640e7bf1bb4140c48ffc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ef4d8767ec9242aead46a3681121934c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8e5d353321d1401e93cbd010c5b0f93d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_393a046c5767439e91607696b821f131",
              "IPY_MODEL_c23da8b720bc4110a74fc06418c8f72d"
            ]
          }
        },
        "8e5d353321d1401e93cbd010c5b0f93d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "393a046c5767439e91607696b821f131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b3300b8ba784476891d213cb7c2124be",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_23a67390a62244689f36c67a18cbfd56"
          }
        },
        "c23da8b720bc4110a74fc06418c8f72d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d2189a552c8f4d868359da406b6b9d61",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1999642624/? [06:38&lt;00:00, 5155366.40it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1fc04ab112e34c77a1823d5a5ae5295e"
          }
        },
        "b3300b8ba784476891d213cb7c2124be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "23a67390a62244689f36c67a18cbfd56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d2189a552c8f4d868359da406b6b9d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1fc04ab112e34c77a1823d5a5ae5295e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiI9H5W_U9QH",
        "colab_type": "code",
        "outputId": "7c5b459c-b113-4ddf-c06b-6faef65d1d3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.image as mpimg\n",
        "from scipy import ndimage\n",
        "from scipy.ndimage.filters import convolve\n",
        "from scipy import misc\n",
        "import scipy.misc as sm\n",
        "import cv2\n",
        "from skimage.feature import canny\n",
        "from IPython.display import HTML\n",
        "from tqdm import tqdm_notebook\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/My\\ Drive/\n",
        "device = 'cuda'\n",
        "torch.cuda.manual_seed(7)\n",
        "torch.manual_seed(7)\n",
        "np.random.seed(7)\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiyquKqWemWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Attention Blocks ###\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_channels_in, reduction_ratio, kernel_size):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.n_channels_in = n_channels_in\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        self.channel_attention = ChannelAttention(n_channels_in, reduction_ratio)\n",
        "        self.spatial_attention = SpatialAttention(kernel_size)\n",
        "\n",
        "    def forward(self, f):\n",
        "        chan_att = self.channel_attention(f)\n",
        "        # print(chan_att.size())\n",
        "        fp = chan_att * f\n",
        "        # print(fp.size())\n",
        "        spat_att = self.spatial_attention(fp)\n",
        "        # print(spat_att.size())\n",
        "        fpp = spat_att * fp\n",
        "        # print(fpp.size())\n",
        "        return fpp\n",
        "\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        assert kernel_size % 2 == 1, \"Odd kernel size required\"\n",
        "        self.conv = nn.Conv2d(in_channels = 2, out_channels = 1, kernel_size = kernel_size, padding= int((kernel_size-1)/2))\n",
        "        # batchnorm\n",
        "\n",
        "    def forward(self, x):\n",
        "        max_pool = self.agg_channel(x, \"max\")\n",
        "        avg_pool = self.agg_channel(x, \"avg\")\n",
        "        pool = torch.cat([max_pool, avg_pool], dim = 1)\n",
        "        conv = self.conv(pool)\n",
        "        # batchnorm ????????????????????????????????????????????\n",
        "        conv = conv.repeat(1,x.size()[1],1,1)\n",
        "        att = torch.sigmoid(conv)        \n",
        "        return att\n",
        "\n",
        "    def agg_channel(self, x, pool = \"max\"):\n",
        "        b,c,h,w = x.size()\n",
        "        x = x.view(b, c, h*w)\n",
        "        x = x.permute(0,2,1)\n",
        "        if pool == \"max\":\n",
        "            x = F.max_pool1d(x,c)\n",
        "        elif pool == \"avg\":\n",
        "            x = F.avg_pool1d(x,c)\n",
        "        x = x.permute(0,2,1)\n",
        "        x = x.view(b,1,h,w)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, n_channels_in, reduction_ratio):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.n_channels_in = n_channels_in\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.middle_layer_size = int(self.n_channels_in/ float(self.reduction_ratio))\n",
        "\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Linear(self.n_channels_in, self.middle_layer_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.middle_layer_size, self.n_channels_in)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        kernel = (x.size()[2], x.size()[3])\n",
        "        avg_pool = F.avg_pool2d(x, kernel )\n",
        "        max_pool = F.max_pool2d(x, kernel)\n",
        "\n",
        "        \n",
        "        avg_pool = avg_pool.view(avg_pool.size()[0], -1)\n",
        "        max_pool = max_pool.view(max_pool.size()[0], -1)\n",
        "        \n",
        "\n",
        "        avg_pool_bck = self.bottleneck(avg_pool)\n",
        "        max_pool_bck = self.bottleneck(max_pool)\n",
        "\n",
        "        pool_sum = avg_pool_bck + max_pool_bck\n",
        "\n",
        "        sig_pool = torch.sigmoid(pool_sum)\n",
        "        sig_pool = sig_pool.unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "        out = sig_pool.repeat(1,1,kernel[0], kernel[1])\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2zqbH9oVE_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Coach Network #####\n",
        "\n",
        "class coach_network(nn.Module):\n",
        "  def __init__(self, drop_ratio = 0.5):\n",
        "    super(coach_network,self).__init__()\n",
        "    model = models.resnet18()\n",
        "    self.layer1 = torch.nn.Sequential(*(list(model.children())[:2]))\n",
        "    self.layer2 = torch.nn.Sequential(*(list(model.children())[4:-2]))\n",
        "    self.drop_ratio = drop_ratio\n",
        "    self.mu = nn.Conv2d(512,100,kernel_size = 1)\n",
        "    self.std = nn.Conv2d(512,100,kernel_size = 1)\n",
        "    self.pred = nn.Conv2d(100,1,kernel_size = 1)\n",
        "    self.upsample = nn.Upsample(scale_factor=16, mode = 'nearest')\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    for m in self.modules():\n",
        "        if isinstance(m, nn.Conv2d) and m is not self.mu and m is not self.std:\n",
        "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.normal_(0, math.sqrt(2. / n))\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            m.weight.data.fill_(1)\n",
        "            m.bias.data.zero_()\n",
        "\n",
        "\n",
        "  def reparametrize(self,mu,logvar):\n",
        "    std = logvar.mul_(0.5).exp_()\n",
        "    eps = Variable(std.data.new(std.size()).normal_())\n",
        "    return eps.mul(std).add_(mu)\n",
        "\n",
        "\n",
        "  def get_features(self,x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    mu = self.mu(x)\n",
        "    logvar = self.std(x)\n",
        "    z = self.reparametrize(mu,logvar)\n",
        "    d = self.pred(z)\n",
        "\n",
        "    return d,mu,logvar\n",
        "\n",
        "  \n",
        "  def forward(self, x, alpha, use_coach = True):\n",
        "    features = None\n",
        "    mu = None\n",
        "    logvar = None\n",
        "\n",
        "    if not use_coach:\n",
        "      size_ = x.size()\n",
        "      features = Variable(torch.rand(size_[0], 1, int(size_[2]/16), int(size_[3]/16)).cuda())\n",
        "    \n",
        "    else:\n",
        "      features,mu,logvar = self.get_features(x)\n",
        "\n",
        "    size_ = features.size()\n",
        "    features = features.view(size_[0], size_[1], size_[2]*size_[3])\n",
        "    p,_ = features.topk(k = int(size_[2]*size_[3]*self.drop_ratio), dim = 2)\n",
        "    partitions = p[:,:, -1]\n",
        "    partitions = partitions.unsqueeze(2).expand(size_[0], size_[1], size_[2]*size_[3])\n",
        "    mask = self.sigmoid(alpha*(features - partitions))\n",
        "\n",
        "\n",
        "    mask = mask.view(size_)\n",
        "\n",
        "    if not self.training:\n",
        "        mask = (mask>0.5).float()\n",
        "\n",
        "    mask = self.upsample(mask)\n",
        "\n",
        "    return mask, mu, logvar\n",
        "\n",
        "\n",
        "##### Inpainting Model #####\n",
        "\n",
        "class residual_block_edge(nn.Module):\n",
        "  def __init__(self,ngpu):\n",
        "    super(residual_block_edge,self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "    self.layer1 = nn.Sequential(nn.ReflectionPad2d(1), nn.utils.spectral_norm(nn.Conv2d(256,256,3,1,padding = 0)), nn.InstanceNorm2d(256), nn.ReLU(True))\n",
        "    self.layer2 = nn.Sequential(nn.ReflectionPad2d(1), nn.utils.spectral_norm(nn.Conv2d(256,256,3,1,padding = 0)), nn.InstanceNorm2d(256))\n",
        "\n",
        "  def forward(self,x):\n",
        "    y = self.layer1(x)\n",
        "    y = self.layer2(y)\n",
        "    out = x + y\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "class residual_block_completion(nn.Module):\n",
        "  def __init__(self,ngpu):\n",
        "    super(residual_block_completion,self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "    self.layer1 = nn.Sequential(nn.ReflectionPad2d(1),nn.Conv2d(256,256,3,1,padding = 0), nn.InstanceNorm2d(256), nn.ReLU(True))\n",
        "    self.layer2 = nn.Sequential(nn.ReflectionPad2d(1),nn.Conv2d(256,256,3,1,padding = 0), nn.InstanceNorm2d(256))\n",
        "\n",
        "  def forward(self,x):\n",
        "    y = self.layer1(x)\n",
        "    y = self.layer2(y)\n",
        "    out = x + y\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class generator1(torch.nn.Module):\n",
        "  def __init__(self, ngpu):\n",
        "    super(generator1,self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "    self.c64 = nn.Sequential(nn.ReflectionPad2d(3),nn.utils.spectral_norm(nn.Conv2d(3,64,7,1, padding = 0)), nn.InstanceNorm2d(64), nn.ReLU(True))\n",
        "    self.cbam64 = CBAM(64,1,7)\n",
        "    self.d128 = nn.Sequential(nn.utils.spectral_norm(nn.Conv2d(64,128,4,2,padding = 1)), nn.InstanceNorm2d(128),nn.ReLU(True))\n",
        "    self.cbam128 = nn.CBAM(128,1,5)\n",
        "    self.d256 = nn.Sequential(nn.utils.spectral_norm(nn.Conv2d(128,256,4,2,padding =1)), nn.InstanceNorm2d(256),nn.ReLU(True))\n",
        "    self.cbam256 = CBAM(256, 1, 3)\n",
        "    self.r256 = residual_block_edge(ngpu)\n",
        "    self.u128 = nn.Sequential(nn.utils.spectral_norm(nn.ConvTranspose2d(256,128,4,2,padding = 1)), nn.InstanceNorm2d(128), nn.ReLU(True))\n",
        "    self.u64 = nn.Sequential(nn.utils.spectral_norm(nn.ConvTranspose2d(128,64,4,2,padding = 1)), nn.InstanceNorm2d(64), nn.ReLU(True))\n",
        "    self.out = nn.Sequential(nn.ReflectionPad2d(3), nn.utils.spectral_norm(nn.Conv2d(64,1,7,1, padding = 0)))\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.c64(x)\n",
        "    x = self.cbam64(x)\n",
        "    x = self.d128(x)\n",
        "    x = self.cbam128(x)\n",
        "    x = self.d256(x)\n",
        "    x = self.cbam256(x)\n",
        "\n",
        "    for i in range(8):\n",
        "      x = self.r256(x)\n",
        "    \n",
        "    x = self.u128(x)\n",
        "    x = self.u64(x)\n",
        "    x = self.out(x)\n",
        "    out = torch.sigmoid(x)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class generator2(torch.nn.Module):\n",
        "  def __init__(self,ngpu):\n",
        "    super(generator2,self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "    self.c64 = nn.Sequential(nn.ReflectionPad2d(3),nn.Conv2d(4,64,7,1,padding = 0), nn.InstanceNorm2d(64), nn.ReLU(True))\n",
        "    self.cbam64 = CBAM(64,1,7)\n",
        "    self.d128 = nn.Sequential(nn.Conv2d(64,128,4,2,padding = 1),nn.InstanceNorm2d(128),nn.ReLU(True))\n",
        "    self.cbam128 = CBAM(128,1,5)\n",
        "    self.d256 = nn.Sequential(nn.Conv2d(128,256,4,2,padding = 1),nn.InstanceNorm2d(256),nn.ReLU(True))\n",
        "    self.cbam256 = CBAM(256,1,3)\n",
        "    self.r256 = residual_block_completion(ngpu)\n",
        "    self.u128 = nn.Sequential(nn.ConvTranspose2d(256,128,4,2,padding = 1), nn.InstanceNorm2d(128), nn.ReLU(True))\n",
        "    self.u64 = nn.Sequential(nn.ConvTranspose2d(128,64,4,2,padding = 1),nn.InstanceNorm2d(64), nn.ReLU(True))\n",
        "    self.out = nn.Sequential(nn.ReflectionPad2d(3), nn.Conv2d(64,3,7,1,padding = 0))\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.c64(x)\n",
        "    x = self.cbam64(x)\n",
        "    x = self.d128(x)\n",
        "    x = self.cbam128(x)\n",
        "    x = self.d256(x)\n",
        "    x = self.cbam256(x)\n",
        "\n",
        "    for i in range(8):\n",
        "      x = self.r256(x)\n",
        "\n",
        "    x = self.u128(x)\n",
        "    x = self.u64(x)\n",
        "    x = self.out(x)\n",
        "    out = torch.tanh(x)\n",
        "    return out\n",
        "\n",
        "class discriminator(torch.nn.Module):\n",
        "  def __init__(self,in_channels,ngpu):\n",
        "    super(discriminator,self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "    self.c1 = nn.Sequential(nn.utils.spectral_norm(nn.Conv2d(in_channels,64,4,2,padding = 1)), nn.LeakyReLU(0.2))\n",
        "    self.c2 = nn.Sequential(nn.utils.spectral_norm(nn.Conv2d(64,128,4,2,padding = 1)), nn.LeakyReLU(0.2))\n",
        "    self.c3 = nn.Sequential(nn.utils.spectral_norm(nn.Conv2d(128,256,4,2,padding = 1)), nn.LeakyReLU(0.2))\n",
        "    self.c4 = nn.Sequential(nn.utils.spectral_norm(nn.Conv2d(256,512,4,1,padding = 1)), nn.LeakyReLU(0.2))\n",
        "    self.c5 = nn.Sequential(nn.utils.spectral_norm(nn.Conv2d(512,1,4,1,padding = 1)))\n",
        "\n",
        "  def forward(self,x):\n",
        "    c1 = self.c1(x)\n",
        "    c2 = self.c2(c1)\n",
        "    c3 = self.c3(c2)\n",
        "    c4 = self.c4(c3)\n",
        "    c5 = self.c5(c4)\n",
        "    out = torch.sigmoid(c5)\n",
        "    return out, [c1, c2, c3, c4, c5]\n",
        "\n",
        "\n",
        "\n",
        "class Vgg19(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Vgg19, self).__init__()\n",
        "        features = list(torchvision.models.vgg19(pretrained = True).features)[:31]\n",
        "        self.features = nn.ModuleList(features).eval() \n",
        "        \n",
        "    def forward(self, x):\n",
        "        results = []\n",
        "        for ii,model in enumerate(self.features):\n",
        "            x = model(x)\n",
        "            if ii in {1,6,11,20,29}:\n",
        "                results.append(x)\n",
        "        return results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "  classname = m.__class__.__name__\n",
        "  if hasattr(m, 'weight') and (classname.find('Conv') != -1):\n",
        "    if hasattr(m, 'bias') and m.bias is not None:\n",
        "      nn.init.constant_(m.bias.data, 0.0)\n",
        "    else:\n",
        "      nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "  elif classname.find('BatchNorm2d') != -1:\n",
        "    nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "    nn.init.constant_(m.bias.data, 0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NenR-L5bVFE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rgbtogray(image): #function that converts an rgb to grayscale image\n",
        "  r,g,b = image[:,0,:,:], image[:,1,:,:], image[:,2,:,:]\n",
        "  gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
        "  gray = gray.unsqueeze_(1)\n",
        "  return gray\n",
        "\n",
        "def regular_mask_grayscale(img,random_num):  #function that creates a square mask randomly that masks 25% of the image pixels\n",
        "  k = 0\n",
        "  b_size = img.size(0)\n",
        "  mask_1 = torch.zeros((b_size,1,256,256))\n",
        "  masked_image_1 = torch.zeros((b_size,1,256,256))\n",
        "  for image in img:\n",
        "    masked_image = image.squeeze_(0)*1\n",
        "    rand = random_num\n",
        "    mask = torch.zeros((256,256))\n",
        "    for i in range(rand-10,rand+11):\n",
        "      for j in range(rand-10,rand+11):\n",
        "        masked_image[i][j] = 1.0\n",
        "        mask[i][j] = 1.0\n",
        "    masked_image = masked_image.unsqueeze_(0)\n",
        "    mask = mask.unsqueeze_(0)\n",
        "    mask_1[k,:,:,:] = mask\n",
        "    masked_image_1[k,:,:,:] = masked_image\n",
        "    k = k+1\n",
        "  return masked_image_1,mask_1\n",
        "\n",
        "def irregular_mask(img):  #function that randomly masks 30% of the image pixels\n",
        "  mask = np.zeros((256,256))\n",
        "  for i in range(140):\n",
        "    for j in range(140):\n",
        "      mask[i][j] = 1.0\n",
        "  mask = mask.ravel()\n",
        "  np.random.shuffle(mask)\n",
        "  mask = mask.reshape((1,256,256))\n",
        "  masked_img = img*1\n",
        "  masked_img = np.maximum(masked_img,mask)\n",
        "  return masked_img, mask\n",
        "\n",
        "def gram_matrix(image):\n",
        "\n",
        "  b,ch, h, w = image.size()\n",
        "  f = image.view(b,ch, w * h)\n",
        "  f_T = f.transpose(1,2)\n",
        "  G = f.bmm(f_T) / (h * w * ch)\n",
        "\n",
        "  return G\n",
        "\n",
        "def canny_edge_detector(image):\n",
        "  b_size = image.size(0)\n",
        "  temp1 = torch.zeros((b_size,1,256,256))\n",
        "  i = 0\n",
        "  for images in image:\n",
        "    temp = images.squeeze_(0)\n",
        "    canny_edge = canny(temp.cpu().numpy())\n",
        "    canny_edge = torch.from_numpy(canny_edge).unsqueeze_(0)\n",
        "    temp1[i,:,:,:] = canny_edge\n",
        "    i = i+1\n",
        "  return temp1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkPsZyP7VFJ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "78bf9d21e056484d8c45beedd50a6862",
            "98d9db8826d24817a6603130f699aaed",
            "448d38e984794a698bc9ecc7236533ed",
            "751e34b8783e4b5e9626ed5c042b6f73",
            "4336e65e1db948dfb8d9b04521ab08af",
            "cfdbc7486f8340e0aa66e4670c4af66d",
            "ba57aec5e6d24d718707555f2666e723",
            "cf9cabb7482640e7bf1bb4140c48ffc2",
            "ef4d8767ec9242aead46a3681121934c",
            "8e5d353321d1401e93cbd010c5b0f93d",
            "393a046c5767439e91607696b821f131",
            "c23da8b720bc4110a74fc06418c8f72d",
            "b3300b8ba784476891d213cb7c2124be",
            "23a67390a62244689f36c67a18cbfd56",
            "d2189a552c8f4d868359da406b6b9d61",
            "1fc04ab112e34c77a1823d5a5ae5295e"
          ]
        },
        "outputId": "d1e33dfe-ca9a-42b2-e9b0-3c7e4555b328"
      },
      "source": [
        "train_dataset = dset.VOCSegmentation('VOC_train', download = True)\n",
        "val_dataset = dset.VOCSegmentation('VOC_val', download = True, image_set = 'val')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to VOC_train/VOCtrainval_11-May-2012.tar\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78bf9d21e056484d8c45beedd50a6862",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to VOC_val/VOCtrainval_11-May-2012.tar\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef4d8767ec9242aead46a3681121934c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2w_e8_yVFMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class voc_dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, dataset,erase_shape = [16,16], erase_count = 16):\n",
        "    self.mean_bgr = np.array([0.4568, 0.4431, 0.4083])\n",
        "    self.std_bgr = np.array([0.2676, 0.2641, 0.2680])\n",
        "    self.dataset = dataset\n",
        "    self.erase_shape = erase_shape\n",
        "    self.erase_count = erase_count\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    image,mask = self.dataset.__getitem__(index)\n",
        "    \n",
        "    image = image.resize((256,256))\n",
        "    image = np.array(image)\n",
        "\n",
        "    mask = mask.resize((256,256))\n",
        "    mask = np.array(mask)\n",
        "    mask = mask.reshape((256,256,1))\n",
        "    mask = np.repeat(mask, 3, axis = 2)\n",
        "    mask = mask/255\n",
        "    mask = mask > 0\n",
        "    mask = mask.astype(np.float)\n",
        "    mask = 1.0 - mask\n",
        "\n",
        " ## Flip the image with prob = 0.5 \n",
        "    flip = torch.LongTensor(1).random_(0,2)[0]*2 - 1\n",
        "    image = image[:, ::flip, :]\n",
        "    mask = mask[:, ::flip, :]\n",
        "\n",
        "## Rotate the image randomly\n",
        "    choice = torch.LongTensor(1).random_(0,4)[0]\n",
        "    angles = [0, 90, 180, 270]\n",
        "    angle = angles[choice]\n",
        "    center = tuple(np.array(image.shape)[:2]/2)\n",
        "    rot_mat = None\n",
        "    rot_mat = cv2.getRotationMatrix2D(center,angle,1)\n",
        "    image = cv2.warpAffine(image, rot_mat, image.shape[:2], flags = cv2.INTER_LINEAR)\n",
        "    mask = cv2.warpAffine(mask, rot_mat, mask.shape[:2], flags = cv2.INTER_LINEAR)\n",
        "    if self.erase_count == 1:                             ### erase a patch in the center of image\n",
        "        offset = (image.shape[0] - erase_shape[0])/2\n",
        "        end = offset+erase_shape[0]\n",
        "        mask[offset:end, offset:end, :] = 0\n",
        "        \n",
        "    else:   \n",
        "        for c_ in range(self.erase_count):\n",
        "            row = torch.LongTensor(1).random_(0, image.shape[0]-self.erase_shape[0]-1)[0]\n",
        "            col = torch.LongTensor(1).random_(0, image.shape[1]-self.erase_shape[1]-1)[0]\n",
        "            \n",
        "            mask[row:row+self.erase_shape[0], col:col+self.erase_shape[1], :] = 0\n",
        "\n",
        "    input_, mask, target = self.transform(image,mask)\n",
        "\n",
        "    return input_, mask, target\n",
        "\n",
        "  def transform(self, image, mask):\n",
        "    image = image/255\n",
        "    image = image.astype(np.float64)\n",
        "    image -= self.mean_bgr\n",
        "            \n",
        "    input_ = image.copy()                        \n",
        "    \n",
        "    image[:,:,0] /= 3*self.std_bgr[0]\n",
        "    image[:,:,1] /= 3*self.std_bgr[1]\n",
        "    image[:,:,2] /= 3*self.std_bgr[2]\n",
        "\n",
        "    index_ = image > 1\n",
        "    image[index_] = 1\n",
        "    index_ = image < -1\n",
        "    image[index_] = -1\n",
        "    image = image.transpose(2, 0, 1)\n",
        "    image = torch.from_numpy(image.copy()).float()\n",
        "    \n",
        "    input_ = input_.transpose(2, 0, 1)\n",
        "    input_ = torch.from_numpy(input_.copy()).float()                        \n",
        "    \n",
        "    mask = mask.transpose(2, 0, 1)\n",
        "    mask = torch.from_numpy(mask.copy())                        \n",
        "\n",
        "    return input_, mask, image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMoZMb63VFRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "erase_shape = [16,16]\n",
        "erase_count = 16\n",
        "mean_bgr = np.array([0.4568, 0.4431, 0.4083])\n",
        "std_bgr = np.array([0.2676, 0.2641, 0.2680])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(voc_dataset(dataset = train_dataset, erase_shape = erase_shape, erase_count = erase_count), batch_size = 8, shuffle = True, num_workers = 1)\n",
        "\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(voc_dataset(dataset = val_dataset, erase_shape = erase_shape, erase_count = erase_count),batch_size = 8, num_workers = 1, shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WLjb3_9VFPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coach_net = coach_network(drop_ratio = 0.75)\n",
        "\n",
        "edge_generator = generator1(1).to(device)\n",
        "inpaint_generator = generator2(1).to(device)\n",
        "edge_discriminator = discriminator(2,1).to(device)\n",
        "feature_matching_discriminator = discriminator(1,1).to(device)\n",
        "inpaint_discriminator = discriminator(4,1).to(device)\n",
        "vgg19 = Vgg19().to(device)\n",
        "\n",
        "\n",
        "use_coach = True\n",
        "coach_optimizer = None\n",
        "inpainting_optimizer = None\n",
        "best_loss = 1e5\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "coach_loss = []\n",
        "rec_weight = 0.99\n",
        "\n",
        "G1_losses = []\n",
        "D1_losses = []\n",
        "G2_losses = []\n",
        "D2_losses = []\n",
        "iters = 0\n",
        "lr1 = 0.0001\n",
        "beta1 = 0\n",
        "beta2 = 0.9\n",
        "criterion = nn.BCELoss()\n",
        "l1criterion = nn.L1Loss()\n",
        "\n",
        "edge_g_opt = optim.Adam(edge_generator.parameters(), lr=lr1, betas = (beta1, beta2))\n",
        "edge_d_opt = optim.Adam(edge_discriminator.parameters(), lr=lr1/10, betas=(beta1, beta2))\n",
        "inpaint_d_opt = optim.Adam(inpaint_discriminator.parameters(), lr=lr1/10, betas=(beta1, beta2))\n",
        "inpaint_g_opt = optim.Adam(inpaint_generator.parameters(), lr=lr1, betas=(beta1, beta2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwa6O9nwVFDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_inpainting(epoch, edge_generator, inpainting_generator, edge_discriminator, inpainting_discriminator, feature_matching_discriminator, vgg19, edge_g_opt, edge_d_opt, inpaint_g_opt, inpaint_d_opt, coach = None, use_coach_masks = False):\n",
        "  inpainting_generator.train()\n",
        "  edge_generator.train()\n",
        "  inpainting_discriminator.train()\n",
        "  edge_discriminator.train()\n",
        "  \n",
        "  if coach is not None:\n",
        "    coach.eval()\n",
        "\n",
        "  for batch_idx, (inputs_, masks, targets) in enumerate(train_loader):\n",
        "    edge_g_opt.zero_grad()\n",
        "    inpaint_g_opt.zero_grad()\n",
        "    edge_d_opt.zero_grad()\n",
        "    inpaint_d_opt.zero_grad()\n",
        "\n",
        "    inputs_, mask, targets = Variable(inputs_.to(device)), Variable(masks.to(device)), Variable(targets.to(device))\n",
        "\n",
        "    b_size = inputs_.size(0)\n",
        "    real_cpu = inputs_\n",
        "\n",
        "    if coach is not None:\n",
        "      mask, _, _ = coach.forward(inputs_,alpha = 1.0, use_coach = use_coach_masks)\n",
        "\n",
        "    real_label = torch.full((b_size,1,30,30),1,device = device)  #label of ones\n",
        "    fake_label = torch.full((b_size,1,30,30),0,device = device)  #label of zeros\n",
        " \n",
        "    I_gray = rgbtogray(real_cpu).to(device)  #converting input image into grayscale\n",
        "    C_gt = canny_edge_detector(I_gray) #edge map of the input image\n",
        "    C_gt = C_gt.type(torch.FloatTensor).to(device)\n",
        "    C_gt_1 = C_gt\n",
        "    I_gray_masked = I_gray*mask #masked grayscale input and mask\n",
        "    I_gray_masked = I_gray_masked.type(torch.FloatTensor).to(device)\n",
        "    C_gt_masked = canny_edge_detector(I_gray_masked)  #masked edge map\n",
        "    C_gt_masked = C_gt_masked.type(torch.FloatTensor).to(device)\n",
        "    edge_generator_input = torch.cat((I_gray_masked, C_gt_masked, mask),1)  #concatenating all the 3 inputs to form the final input to edge generator\n",
        "    C_pred = edge_generator(edge_generator_input)  #predicted edge map by generator, detaching so that gradients dont flow into the generator\n",
        "\n",
        "    true = torch.cat((C_gt,I_gray),1).to(device)\n",
        "    fake = torch.cat((C_pred,I_gray),1).to(device)\n",
        "\n",
        "    true_probability, [c1,c2,c3,c4,c5] = edge_discriminator(true)\n",
        "    fake_probability, [d1,d2,d3,d4,d5] = edge_discriminator(fake.detach())\n",
        "\n",
        "    true_loss = criterion(true_probability, real_label)\n",
        "    true_loss.backward()\n",
        "    fake_loss = criterion(fake_probability, fake_label)\n",
        "    fake_loss.backward()\n",
        "    D1_loss = true_loss + fake_loss\n",
        "    D1_losses.append(D1_loss)\n",
        "    edge_d_opt.step()\n",
        "    \n",
        "    \"\"\"\n",
        "    Updating the G1 network\n",
        "    \"\"\"\n",
        "    edge_generator.train()\n",
        "    edge_g_opt.zero_grad()\n",
        "\n",
        "    C_pred = edge_generator(edge_generator_input)  \n",
        "\n",
        "    true_1 = C_gt_1\n",
        "    fake_1 = C_pred\n",
        "\n",
        "    true_probability_1, [c1,c2,c3,c4,c5] = feature_matching_discriminator(true_1)\n",
        "    fake_probability_1, [d1,d2,d3,d4,d5] = feature_matching_discriminator(fake_1)\n",
        "\n",
        "    #true_loss = criterion(true_probability_1, real_label)\n",
        "    #fake_loss = criterion(fake_probability_1, fake_label)\n",
        "    #edge_discriminator_loss = -(true_loss + fake_loss)\n",
        "\n",
        "    fake_loss_updated = criterion(fake_probability_1,real_label).to(device)\n",
        "\n",
        "    lfm_1 = l1criterion(c1,d1)\n",
        "    lfm_2 = l1criterion(c2,d2)\n",
        "    lfm_3 = l1criterion(c3,d3)\n",
        "    lfm_4 = l1criterion(c4,d4)\n",
        "    lfm_5 = l1criterion(c5,d5)\n",
        "    lfm = lfm_1 + lfm_2 + lfm_3 + lfm_4 + lfm_5\n",
        "\n",
        "    edge_generator_loss = 2*lfm + fake_loss_updated\n",
        "\n",
        "    edge_generator_loss.backward()\n",
        "    edge_g_opt.step()\n",
        "    G1_losses.append(edge_generator_loss)\n",
        "\n",
        "    \"\"\"\n",
        "    Updating the D2 network\n",
        "    \"\"\"\n",
        "    inpaint_discriminator.train()\n",
        "    inpaint_d_opt.zero_grad()\n",
        "\n",
        "    I_gt_masked = real_cpu*mask\n",
        "    \n",
        "    C_pred_1 = C_pred.detach().to(device)\n",
        "    I_gt = real_cpu.detach().to(device)\n",
        "    C_comp = (C_gt_masked + (torch.ones((b_size,1,256,256)).to(device)-mask) * C_pred_1).to(device)\n",
        "    generator2_input = torch.cat((I_gt_masked,C_comp),1).to(device)\n",
        "    I_pred = inpaint_generator(generator2_input).to(device)\n",
        "\n",
        "    true_2 = torch.cat((I_gt,C_comp),1).to(device)\n",
        "    fake_2 = torch.cat((I_pred,C_comp),1).to(device)\n",
        "\n",
        "    true_probability_2, [c1,c2,c3,c4,c5] = inpaint_discriminator(true_2)\n",
        "    fake_probability_2, [d1,d2,d3,d4,d5] = inpaint_discriminator(fake_2.detach())\n",
        "\n",
        "    true_loss_1 = criterion(true_probability_2,real_label).to(device)\n",
        "    true_loss_1.backward()\n",
        "    fake_loss_1 = criterion(fake_probability_2,fake_label).to(device)\n",
        "    fake_loss_1.backward()\n",
        "    D2_loss = (true_loss_1 + fake_loss_1).to(device)\n",
        "    inpaint_d_opt.step()\n",
        "    D2_losses.append(D2_loss)\n",
        "\n",
        "    \"\"\"\n",
        "    Updating the G2 network\n",
        "    \"\"\"\n",
        "    inpaint_generator.train()\n",
        "    inpaint_g_opt.zero_grad()\n",
        "\n",
        "    #L1 loss\n",
        "    I_pred_1 = inpaint_generator(generator2_input).to(device)\n",
        "    inpaint_generator_loss = (1 * l1criterion(I_pred_1, I_gt)).to(device)\n",
        "    \n",
        "\n",
        "    #Adversarial Loss\n",
        "    true_3 = torch.cat((I_gt,C_comp),1).to(device)\n",
        "    fake_3 = torch.cat((I_pred_1,C_comp),1).to(device)\n",
        "\n",
        "    true_probability_3, [c1,c2,c3,c4,c5] = inpaint_discriminator(true_3)\n",
        "    fake_probability_3, [d1,d2,d3,d4,d5] = inpaint_discriminator(fake_3)\n",
        "\n",
        "    #true_loss_2 = criterion(true_probability_3,real_label).to(device)\n",
        "    #fake_loss_2 = criterion(fake_probability_3,fake_label).to(device)\n",
        "    \n",
        "    fake_loss_updated = criterion(fake_probability_3,real_label).to(device)\n",
        "    #inpaint_discriminator_loss = - (true_loss_2 + fake_loss_2)\n",
        "    \n",
        "    inpaint_generator_loss = inpaint_generator_loss + 0.1 * fake_loss_updated\n",
        "\n",
        "    \n",
        "    #Perceptual Loss\n",
        "    [c1,c2,c3,c4,c5] = vgg19(I_pred_1)\n",
        "    [d1,d2,d3,d4,d5] = vgg19(I_gt)\n",
        "\n",
        "    lp_1 = l1criterion(c1,d1).to(device)\n",
        "    lp_2 = l1criterion(c2,d2).to(device)\n",
        "    lp_3 = l1criterion(c3,d3).to(device)\n",
        "    lp_4 = l1criterion(c4,d4).to(device)\n",
        "    lp_5 = l1criterion(c5,d5).to(device)\n",
        "\n",
        "    lp_loss = lp_1 + lp_2 + lp_3 + lp_4 + lp_5\n",
        "    lp_loss = lp_loss/50\n",
        "\n",
        "    inpaint_generator_loss = inpaint_generator_loss + lp_loss\n",
        "\n",
        "\n",
        "    #Style Loss\n",
        "    ls_1 = l1criterion(gram_matrix(c1),gram_matrix(d1)).to(device)\n",
        "    ls_2 = l1criterion(gram_matrix(c2),gram_matrix(d2)).to(device)\n",
        "    ls_3 = l1criterion(gram_matrix(c3),gram_matrix(d3)).to(device)\n",
        "    ls_4 = l1criterion(gram_matrix(c4),gram_matrix(d4)).to(device)\n",
        "    ls_5 = l1criterion(gram_matrix(c5),gram_matrix(d5)).to(device)\n",
        "    \n",
        "    ls_loss = ls_1 + ls_2 + ls_3 + ls_4 + ls_5\n",
        "    ls_loss = 250 * ls_loss\n",
        "\n",
        "    inpaint_generator_loss = inpaint_generator_loss + ls_loss\n",
        "\n",
        "    inpaint_generator_loss.backward()\n",
        "    inpaint_g_opt.step()\n",
        "    G2_losses.append(inpaint_generator_loss)\n",
        "\n",
        "    print('[%d/%d]\\t [%d/51]\\t D1_loss: %.4f\\t G1_loss: %.4f\\t D2_loss: %.4f\\t G2_loss: %.4f\\t' % (epoch,epochs[iter_],i,D1_loss, edge_generator_loss, D2_loss, inpaint_generator_loss))\n",
        "\n",
        "\n",
        "def train_coach(epoch, inpaint_generator, edge_generator, coach_net, coach_optimizer):\n",
        "  coach_net.train()\n",
        "  inpaint_generator.eval()\n",
        "  edge_generator.eval()\n",
        "  coach_loss.append(0)\n",
        "\n",
        "  for batch_idx, (inputs_, masks, targets) in enumerate(train_loader):\n",
        "    coach_optimizer.zero_grad()\n",
        "    inputs_, targets = Variable(inputs_.to(device)), Variable(targets.to(device))\n",
        "\n",
        "    masks, mu, logvar = coach_net.forward(inputs_, alpha = 1)\n",
        "    b_size = inputs_.size(0)\n",
        "\n",
        "    test_cpu = inputs_\n",
        "    I_gray = rgbtogray(test_cpu).cuda()\n",
        "    I_gray_masked = masks*I_gray\n",
        "    I_gray_masked = I_gray_masked.cuda()\n",
        "\n",
        "    I_gt_masked = test_cpu*masks\n",
        "    I_gt_masked = I_gt_masked.cuda()\n",
        "    C_gt_masked = canny_edge_detector(I_gray_masked).cuda()\n",
        "    edge_generator_input = torch.cat((I_gray_masked, C_gt_masked, masks),1)\n",
        "\n",
        "    C_pred = edge_generator(edge_generator_input) ### Output after first generator\n",
        "\n",
        "    C_comp = (C_gt_masked + (torch.ones((b_size,1,256,256)).to(device)-masks) * C_pred).to(device)\n",
        "    generator2_input = torch.cat((I_gt_masked,C_comp),1).to(device)\n",
        "    \n",
        "    I_pred = inpaint_generator(generator2_input).to(device)\n",
        "    \n",
        "    outputs = I_pred\n",
        "    \n",
        "    mse_loss = (outputs - targets)**2\n",
        "    mse_loss = -1*F.threshold(-1*mse_loss, -2, -2)\n",
        "    loss_rec = torch.sum(mse_loss*(1 - masks))/torch.sum(1 - masks)\n",
        "\n",
        "    mu = mu.mean(dim = 2).mean(dim = 2)\n",
        "    logvar = logvar.mean(dim = 2).mean(dim = 2)\n",
        "\n",
        "    KLD = 0 \n",
        "    try:\n",
        "      KLD = -0.5*torch.sum(1+logvar - mu**2 - logvar.exp())\n",
        "    except:\n",
        "      KLD = 0\n",
        "\n",
        "    total_loss = 1 - loss_rec + 1e-6*KLD\n",
        "\n",
        "    total_loss.backward()\n",
        "    coach_optimizer.step()\n",
        "\n",
        "    coach_loss[-1] = coach_loss[-1] + total_loss.data\n",
        "    print('Coach (loss = %.4f)' % (coach_loss[-1]/(batch_idx+1)))\n",
        "  coach_loss[-1] = coach_loss[-1]/len(train_loader)\n",
        "\n",
        "\n",
        "def val_inpainting(iter_, epoch, inpaint_generator, edge_generator, inpaint_discriminator, edge_discriminator, coach = None, use_coach_masks = False):\n",
        "  global best_loss\n",
        "  inpainting_net.eval()\n",
        "\n",
        "  if coach is not None:\n",
        "    coach.eval()\n",
        "  \n",
        "  val_loss.append(0)\n",
        "  for batch_idx, (inputs_, masks, targets) in enumerate(val_loader):\n",
        "    inputs_, masks, targets = Variable(inputs_.cuda()), Variable(masks.cuda().float()), Variable(targets.cuda())\n",
        "\n",
        "    if coach is not None:\n",
        "      masks, _, _ = coach.forward(inputs_, alpha = 100, use_coach = use_coach_masks)\n",
        "\n",
        "    b_size = inputs_.size(0)\n",
        "\n",
        "    test_cpu = inputs_\n",
        "    I_gray = rgbtogray(test_cpu).cuda()\n",
        "    I_gray_masked = masks*I_gray\n",
        "    I_gray_masked = I_gray_masked.cuda()\n",
        "\n",
        "    I_gt_masked = test_cpu*masks\n",
        "    I_gt_masked = I_gt_masked.cuda()\n",
        "    C_gt_masked = canny_edge_detector(I_gray_masked).cuda()\n",
        "    edge_generator_input = torch.cat((I_gray_masked, C_gt_masked, masks),1)\n",
        "\n",
        "    C_pred = edge_generator(edge_generator_input) ### Output after first generator\n",
        "\n",
        "    C_comp = (C_gt_masked + (torch.ones((b_size,1,256,256)).to(device)-masks) * C_pred).to(device)\n",
        "    generator2_input = torch.cat((I_gt_masked,C_comp),1).to(device)\n",
        "    \n",
        "    I_pred = inpaint_generator(generator2_input).to(device)\n",
        "\n",
        "\n",
        "    mse_loss = (targets - outputs_1)**2\n",
        "    mse_loss = (-1*F.threshold(-1*mse_loss, -2, -2))\n",
        "    loss_rec = (torch.sum(mse_loss * (1 - masks))/torch.sum(1 - masks))\n",
        "\n",
        "    outputs_2 = inpainting_net(inputs_*(1 - masks))\n",
        "    mse_loss = (targets - outputs_2)**2\n",
        "    mse_loss = (-1*F.threshold(-1*mse_loss, -2, -2))\n",
        "    loss_con = (torch.sum(mse_loss*masks)/torch.sum(masks))\n",
        "\n",
        "    total_loss = (rec_weight*loss_rec + (1 - rec_weight)*loss_con)\n",
        "\n",
        "    val_loss[-1] = (val_loss[-1] + total_loss.data)\n",
        "    print('Val (loss= %.4f)' % (val_loss[-1]/(batch_idx+1)))\n",
        "\n",
        "  val_loss[-1] = (val_loss[-1]/len(val_loader))\n",
        "\n",
        "  if best_loss > val_loss[-1]:\n",
        "    best_loss = val_loss[-1]\n",
        "    print('Saving...')\n",
        "    state = {'inpaint_generator' : inpaint_generator, 'edge_generator' : edge_generator, 'inpaint_discriminator' : inpaint_discriminator, 'edge_discriminator' : edge_discriminator, 'coach' : coach}\n",
        "\n",
        "    torch.save(state, 'edgeconnectdownsamplingbmvc2018' + str(iter_) + '.ckpt.t7')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WrzmjMVVbvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_coach_masks = False\n",
        "epochs = []\n",
        "lrs = []\n",
        "\n",
        "if use_coach:\n",
        "    epochs = [30, 20, 30, 30, 30, 30]\n",
        "    lrs = [[1e-1, 1e-2, 1e-3, 1e-4],\n",
        "       [1e-5, 1e-5, 1e-5, 1e-5], \n",
        "       [1e-5, 1e-5, 1e-5, 1e-5], \n",
        "       [1e-5, 1e-5, 1e-5, 1e-5],\n",
        "       [1e-5, 1e-5, 1e-5, 1e-5], \n",
        "       [1e-5, 1e-5, 1e-5, 1e-5], \n",
        "       [1e-5, 1e-5, 1e-5, 1e-5], \n",
        "       [1e-5, 1e-5, 1e-5, 1e-5], \n",
        "       [1e-5, 1e-5, 1e-5, 1e-5], \n",
        "       [1e-5, 1e-5, 1e-5, 1e-5], \n",
        "       [1e-5, 1e-5, 1e-5, 1e-5]]\n",
        "else:\n",
        "    epochs = [100]\n",
        "    lrs = [[1e-1, 1e-2, 1e-3, 1e-4]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8empJfEtVb2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for iter_ in range(2):\n",
        "    best_loss = 1e5    \n",
        "    \n",
        "    if use_coach and iter_ > 0:\n",
        "        use_coach_masks = True\n",
        "        \n",
        "        optimizer_coach = optim.Adam(coach_net.parameters(), lr=1e-5)\n",
        "        \n",
        "        for epoch in range(epochs[iter_]):\n",
        "            train_coach(epoch, inpaint_generator = inpaint_generator, edge_generator = edge_generator, coach_net=coach_net, coach_optimizer=optimizer_coach)\n",
        "    \n",
        "    for epoch in range(epochs[iter_]):               \n",
        "        train_inpainting(epoch, edge_generator = edge_generator, inpainting_generator = inpaint_generator, edge_discriminator = edge_discriminator, inpainting_discriminator = inpaint_discriminator, feature_matching_discriminator = feature_matching_discriminator, vgg19 = vgg19, edge_g_opt = edge_g_opt, edge_d_opt = edge_d_opt, inpaint_g_opt = inpaint_g_opt, inpaint_d_opt = inpaint_d_opt, coach=coach_net, use_coach_masks=use_coach_masks)\n",
        "        val_inpainting(iter_, epoch, inpaint_generator = inapint_generator, edge_generator = edge_generator, inpaint_discriminator = inpaint_discriminator, edge_discriminator = edge_discriminator, coach=coach_net, use_coach_masks=use_coach_masks)\n",
        "        print('Epochs: %d/%d' % (epoch, epochs[iter_]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX8Jpu8WVb0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6Dtz0eHVbyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9xJJ7YsVbtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}